# 🔌 Smart Grid Power Consumption Analyzer  

This repository contains my implementation of a **Power Consumption Analyzer System** as part of my assignment.  
The project simulates a real-world **smart grid monitoring pipeline**, where hourly power consumption is recorded at household level, merged with city-level metadata, and analyzed to generate risk insights and visualizations.  

---

## 📌 Problem Statement  

Responsible for monitoring power consumption across multiple cities to optimize energy distribution and prevent blackouts.  

Each city has multiple districts, and each district has hundreds of households.  
Smart meters in each household record hourly consumption data, stored as:  

- **CSV files** → per district, with hourly household-level consumption.  
- **JSON files** → per city per day, with district metadata (thresholds, critical hours).  

Our system must:  

- **Read & Merge Hybrid Data** (CSV + JSON).  
- **Perform Multi-Level Aggregations** (household, district, city).  
- **Generate Risk Insights** (detect threshold violations & critical-hour peaks).  
- **Handle Missing & Corrupted Data** gracefully.  
- **Generate Output Reports** (CSV + JSON).  
- **Produce Visualizations** for trends and risk distribution.  

---

## Requiremts Setup

pip install -r requirements.txt

---

## Run The project

python main.py

This will:

Generate CSV + JSON data in data/.

Process and merge data.

Compute district/city level statistics.

Save outputs in output/summary_csv/ and output/reports_json/.

---

## Visualize Results

python covalence_assignment_visualization.py

This will:
Display visualizations (bar charts, line plots, pie charts, heatmaps).
Save all plots in output/plots/.

---


## 🏗️ Project Structure  


├── main.py # Main program - generates data, processes metrics, saves outputs

├── covalence_assignment_visualization.ipynb # Visualization script (bar charts, pie charts, heatmaps, etc.)

├── data/


│ ├── csv/ # District-level hourly consumption CSV files


│ └── json/ # City-level metadata JSON files (thresholds, critical hours)


├── output/


│ ├── summary_csv/ # Per-city daily summary CSVs


│ └── reports_json/ # JSON reports with risk alerts


└── README.md # Project documentation (this file)

---


## Explanation of design decisions

✅ Synthetic Data Generation

2+ cities × 3 districts × 10 households × 24 hourly readings × 2 days

CSVs (district-level) + JSONs (city-level) auto-generated by code

✅ Metrics Calculation

Average, Minimum, Maximum consumption

Peak hour detection

Threshold violation count + ratio

Risk score (0–1) and risk level (NO RISK, LOW, MEDIUM, HIGH)

✅ Risk Insights

District flagged “at risk” if ≥ 25% violations or peak hour is in critical hours (18:00–21:00)

Risk score formula:

Risk Score = 0.6 × Violation Ratio + 0.4 × Peak Hour Risk


✅ Automated Reports

CSV summary per city

JSON risk report with critical alerts & recommendations

✅ Visualizations

Bar Chart → Risk level distribution per city

Line Plot → Hourly consumption trends with peak hour marked

Pie Chart → Overall risk level share

Heatmap → Household vs hour consumption

Violation ratio barplot

Boxplot, violin plot, histogram for distribution analysis

Interactive Plotly Map → District-wise risk & average consumption

---


## 🖼️ Sample Outputs

1️⃣ City Summary CSV





city	district_id	avg_consumption	max_consumption	violations	risk_score	risk_level



CityA	     101	         1.43	         3.10	         12	        0.72	     HIGH


2️⃣ City JSON Report
{
  "city": "City A",
  "date": "2025-09-10",
  "summary": {
    "total_districts": 3,
    "high_risk_districts": 1,
    "medium_risk_districts": 1,
    "low_risk_districts": 1
  },
  "critical_alerts": [
    {
      "district_id": "101",
      "Avg % of violations per day": "30%",
      "risk_score": 0.72
    }
  ]
}


